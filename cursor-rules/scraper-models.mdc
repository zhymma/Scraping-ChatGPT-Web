---
description: 
globs: 
alwaysApply: true
---
---
description: This rule provides the description of the possible scraper types that can be created.
globs: **/*.py
---
# Scraper types
Here's a list of possible scraper types and ther data structures

## E-commerce PLP
- The structure of the items in a PLP scraper is the following: website_name, extraction_date, product_code, item_url, full_price, price, currency, image_url, brand, product_category1, product_category2, product_category3, product_name
- When asked to create an e-commerce PLP scraper, set items.py file and in the scraper ouput accordingly to the data structure.
- PLP pages are the product list pages in an e-commerce, also called catalogue pages. In a e-commerce PLP scraper, the scraper should crawl all the product catalog without entering the pages with product details.
- The scraper will usually start from a home page.


## E-commerce PDP
- The strucutre of the items in a PDP scraper is the following: website_name, extraction_date, product_code, item_url, full_price, price, currency, image_url, brand, product_category1, product_category2, product_category3, product_name, product_description, product_size, product_color, additional_info
- When asked to create an e-commerce PDP scraper, set items.py file and in the scraper ouput accordingly to the data structure.
- PDP pages are product detail pages, the final leaf of an e-commerce website. An e-commerce PDP scraper will have in input a list of PDP pages and won't need to crawl the website further.


## Chatbox Conversation (Manual Login + Prompt/Response)
- The structure of the items in a Chatbox Conversation scraper is the following: website_name, extraction_date, conversation_id, item_url, session_user, model_name, mode_online, prompt_text, response_text, response_citations, response_language, latency_ms, status, error_message, message_id, parent_message_id, tokens_prompt, tokens_completion, tokens_total, created_at
- When asked to create a Chatbox Conversation scraper (e.g., ChatGPT、Kimi、Qwen、DeepSeek、Doubao 等), set `items.py` and the scraper output accordingly to the data structure.
- Operator performs login manually outside of the scraper. The scraper will wait for a logged-in session, then (optional) switch online/retrieval mode → input question → send → wait and extract model's response (including citations/sources, etc.). Prefer Camoufox (headless browser, stronger anti-detection).
- **Examples**: `MCPfiles/kimi_moonshot_chat_scraper.py` (Kimi), `MCPfiles/deepseek_chat_scraper.py` (DeepSeek)


# How to fill the scraper fields with values
- **When asked to map a field of a data structure to the information contained in the HTML, use the following rules:**
    - **website_name**: this is a fixed value per each scraper, usually the website's name in upper case. If in doubt, ask to the operator
    - **extraction_date**: fixed value for the whole execution, YYYY-MM-DD format. Use datetime library
    - **product_code**: code that identifies every single product on the website
    - **item_url**: URL of the page containing the details of the product. If PDP data structure, it corresponds to response.url
    - **full_price**: price before the discounts. If there's no discount on the item, it's the selling price.
    - **price**: final selling price after the discounts. If no discount is on the website, it's the selling price. 
    - **currency**: ISO3 Code for currency, fixed value for a whole scraper. Detect the currency from the HTML and use the ISO Code to populate the field
    - **brand**: brand or producer of the product sold on the website
    - **product_category1**: first level or product categorization, usually the first level of the breadcrumb of the page, if any.
    - **product_category2**: second level or product categorization, usually the second level of the breadcrumb of the page, if any.
    - **product_category3**: third level or product categorization, usually the third level of the breadcrumb of the page, if any.
    - **product_name**: name of the product as shown on the pages
- In any case and in any field of a scraper, do not hardcode any value but always find a selector to get the correct one. 
- Always print in output every field of the scraper, even if it's empty.

# How to create an e-commerce PLP scraper
Follow this step by step guide to create the code of the scraper. Before doing so, be sure to have read and executed all the commands in the [prerequisites.mdc](mdc:.cursor/rules/prerequisites.mdc) rule
- **Create a scrapy project**: create a scrapy project named like the website. Follow the file structure you find in the [scrapy.mdc](mdc:.cursor/rules/scrapy.mdc) file
- **Download the HTML of the home page**: download the home page of the website in the file homepage.html, using the full path retrieved before
- **Read the HTML of the home page**: after downloading the home page HTML, read the file homepage.html to understand where are located the URLs of the product categories
- **Ask in chat the URL of a category**: after reading the homepage.html file, ask in chat an URL of a product category. 
- **Write the parse method**: write the XPATH selectors needed for extract the URL given in chat and all the URLs that are similar. Do not extract strictly the URL passed but all the URLs with the same level of hierarchy in the HTML. Per each URL then call a method called parse_category
- **Download the HTML of the product category page**: download the HTML of the product category page in a file called product_category.html in the full path retrieved in the previous steps
- **Interpret the HTML file**: strip down all the css code from the file. Check if there is a complete JSON inside the HTML page, that can be used instead of parsing the HTML. Write in the chat if you've found it or not. 
- **Elaborate the JSON, if found**: if a JSON is found, fill the output fields with the values extracted from the JSON. Use the json python package. 
- **Read the product category HTML**: if a JSON is not found, read the file product_category.html to find where is the data needed to fill all the output fields
- **Write XPATH selectors of the items**: if a JSON is not found, write the XPATH selectors needed to extract the information needed to fill all the fields of the output items, based on the HTML of the product_category.html file
- **Write the code of the method parse_category**: now you can complete the parse_category method by using the XPATH or the JSON fields for retrieving the data needed for the items 

# How to create an e-commerce PDP scraper
Follow this step by step guide to create the code of the scraper. 
- **Create a scrapy project**: create a scrapy project named like the website. Follow the file structure you find in the @scrapy.mdc file
- **Download the HTML of a pdp page**: download the HTML of a PDP page of the website in the file pdp_page.html, using the full path retrieved before
- **Read the HTML of the PDP page**: after downloading the PDP page HTML, read the file pdp_page.html to understand where are located the information needed to fill the fields of the e-commerce PDP data structure
- **Write the parse_product method**: write the XPATH selectors needed for extract the information. Per each URL yield an item in output
- **Create the final structire**: the E-commerce PDP scraper will receive PDP pages in input from a file called input.txt. When asked to write a PDP scraper, first load the URLS contained in the input.txt file in a list called urls and then iterate it. Per each url, call the method parse_product where you will write the selectors and yield the items

# How to create a Chatbox Conversation scraper (Kimi Example)

## Overview
This guide describes creating a Chatbox Conversation scraper for AI chat services like Kimi.moonshot.cn, ChatGPT, Qwen, etc. The scraper uses Camoufox (stealth browser automation) to simulate real user interactions, handle manual login, send prompts, and extract streaming responses with citations.

## Architecture & Data Flow

### Input Files Structure
```
project_root/
├── task1_input_prompts.txt    # Task 1 prompts (one per line)
├── task2_input_prompts.txt    # Task 2 prompts
└── MCPfiles/
    └── kimi_moonshot_chat_scraper.py
```

### Output Files Structure
```
output/
├── kimi_conversations_task1.ndjson    # Task 1 structured data (JSON Lines)
└── kimi_conversations_task1.md        # Task 1 human-readable format
```

### Data Structure
Each conversation item contains:
- **website_name**: Fixed value (e.g., "KIMI")
- **conversation_id**: Extracted from URL
- **item_url**: Full conversation URL
- **model_name**: Detected model name (if visible)
- **mode_online**: Online search mode status
- **prompt_text**: User's input question
- **response_text**: AI's response with inline citations in Markdown format
- **web_search_results**: Array of structured search results `[{href, name, title, snippet, date}]`
- **response_language**: Detected language ("zh"/"en")
- **latency_ms**: Response time in milliseconds
- **status**: "ok" or "error"
- **error_message**: Error details if status is "error"

## Step-by-Step Workflow

### 1. Technology Setup
**Choose Camoufox over Scrapy:**
- Camoufox provides browser automation with strong anti-detection
- Handles JavaScript rendering, streaming responses, and dynamic content
- Supports persistent sessions (cookies, localStorage, sessionStorage)

**Key Settings:**
```python
with Camoufox(
    humanize=True,           # Simulate human behavior
    geoip=False,             # Disable GeoIP (may require extra dependencies)
    locale="zh-CN",          # Set Chinese locale for Kimi
) as browser:
    page = browser.new_page(
        locale="zh-CN",
        extra_http_headers={
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8"
        }
    )
```

### 2. Input Processing
**Multi-Task Support:**
- Automatically scan for `*_input_prompts.txt` files
- Extract task name from filename: `task1_input_prompts.txt` → `task1`
- Generate task-specific output files: `kimi_conversations_task1.ndjson`

**Prompt Reuse Detection:**
- Load existing NDJSON file before processing
- Build a set of already-processed `prompt_text` values
- Skip prompts that have `status != "error"` in existing data
- Only process new or previously-failed prompts

### 3. Browser Initialization & Login
**Session Persistence:**
- Save/load cookies to `kimi_cookies.json`
- Save/load localStorage/sessionStorage to `kimi_storage.json`
- Navigate to chat URL: `https://kimi.moonshot.cn/chat`

**Manual Login Flow:**
```python
def wait_for_login(page, timeout_seconds=300):
    print("[INFO] Please login within specified time...")
    time.sleep(15)  # Give user time to login
    
    # Check for chat input box presence
    for selector in CHAT_INPUT_SELECTORS:
        if pick_first_visible(page, selector):
            return True
    return False
```

**Maximize Window:**
- Detect monitor resolution using `screeninfo`
- Maximize browser window for better visibility during debugging

### 4. Main Processing Loop
**For Each Prompt:**

#### 4.1 Send Prompt
```python
# Focus input box
input_box = pick_first_visible(page, CHAT_INPUT_SELECTORS)
input_box.click()
input_box.type(prompt_text)

# Click send button
send_btn = pick_first_visible(page, SEND_BUTTON_SELECTORS)
send_btn.click()
```

#### 4.2 Wait for Response Completion
**Detection Logic:**
- Monitor "Stop" icon visibility (generation in progress)
- Check "Send" icon visibility (generation complete)
- Track text stability (no changes for N ticks)
- Fallback: 10-second no-change timeout

**Completion Criteria:**
```python
if (not is_generating(page)) and 
   is_send_icon_visible(page) and 
   (stable_ticks >= required_stable_ticks) and 
   len(text) > 0:
    break
```

**Button State Detection:**
- `STOP_ICON_SELECTOR`: `'div.send-button svg[name="stop"]'`
- `SEND_ICON_SELECTOR`: `'div.send-button svg[name="Send"]'`
- `SEND_BUTTON_CONTAINER_DISABLED`: `"div.send-button-container.disabled"`

#### 4.3 Extract Response Text
**Inline Citation Handling:**
1. Find all `.rag-tag` elements in response
2. Use JavaScript to trigger `mouseover` events on all tags
   ```javascript
   tags.forEach(tag => {
       tag.dispatchEvent(new MouseEvent('mouseover', {...}));
       tag.dispatchEvent(new MouseEvent('mouseenter', {...}));
   });
   ```
3. Wait for `div.rag-tag` → `a.rag-tag` transformation (0.5-0.7s)
4. Extract full HTML from markdown container
5. Convert to Markdown with inline links preserved

**HTML to Markdown Conversion:**
```python
def html_to_markdown(html: str) -> str:
    # Convert <a href="..." data-site-name="X">...</a> to [X](url)
    # Convert <br> to \n
    # Convert <li> to "- "
    # Convert <h1-6> to "# " etc.
    # Remove all other tags
    # Return clean Markdown text
```

#### 4.4 Extract Web Search Results (if used)
**Detection:**
- Check for `.container-block` with search icon
- Look for `div.toolcall-title-container` containing "搜索"

**Extraction Process:**
1. Click search result block to open side panel
2. Wait for `div.side-console-container` to appear
3. Extract each result:
   ```python
   {
       "href": "https://...",
       "name": "Site Name",
       "title": "Article Title",
       "snippet": "Preview text...",
       "date": "2025-01-15"
   }
   ```
4. Store as structured JSON array in `web_search_results` field

#### 4.5 Detect Model Info
**Model Name:**
- Search for elements with `aria-label*="模型"` or `aria-label*="Model"`
- Extract visible text

**Online Mode:**
- Search for elements with `aria-label*="联网"` or `aria-pressed`
- Check `aria-pressed="true"` state

#### 4.6 Calculate Latency
```python
send_ts = time.time()
# ... wait for response ...
latency_ms = int((time.time() - send_ts) * 1000)
```

#### 4.7 Save Result Immediately
**Crash-Resistant Design:**
- Write to NDJSON/MD files after EACH prompt completes
- Use append mode if file exists, create mode if new
- Log save confirmation: `"✓ Saved to kimi_conversations_task1.ndjson"`

**Benefits:**
- No data loss if script crashes mid-run
- Real-time progress monitoring
- Supports resume from interruption

#### 4.8 Start New Conversation
**For Next Prompt (if not last):**
```python
if idx < len(prompts) - 1:
    human_think_time(0.7, 1.0)  # Random delay
    click_new_conversation(page)  # Click "新建会话" button
    human_think_time(0.5, 1.0)  # Wait for new conversation load
```

**New Conversation Button Selectors:**
```python
[
    'div.action-label:has(svg[name="AddConversation"])',
    'svg[name="AddConversation"]',
    'button:has-text("新建会话")',
    'div:has-text("新建会话")',
]
```

**Why New Conversations:**
- Each prompt gets a clean context (no previous Q&A interference)
- Unique conversation_id per prompt
- Better data isolation
- Mimics real user behavior

### 5. Session Management
**After Each Task:**
- Save cookies: `save_cookies_from_context(page, SESSION_COOKIES_FILE)`
- Save storage: `save_storage_to_file(page, SESSION_STORAGE_FILE)`
- Close browser gracefully

**Next Run Benefits:**
- Skip login (session preserved)
- Faster startup
- Less detection risk

### 6. Error Handling
**Robust Recovery:**
```python
try:
    item = send_prompt_and_collect(page, prompt, "KIMI")
except Exception as e:
    item = {
        "status": "error",
        "error_message": str(e),
        "prompt_text": prompt,
        # ... other fields with empty/default values
    }
```

**Common Errors:**
- Network timeout → Retry with exponential backoff
- Element not found → Try alternative selectors
- Login expired → Stop and request re-login
- Rate limit → Add longer delays

### 7. Performance Optimizations

**JavaScript Batch Processing:**
- Hover all citation tags at once (not one-by-one)
- Reduce wait times: 0.5-0.7s for API calls vs 2-3s serial hovers
- O(1) time complexity vs O(N) for N citations

**Smart Waiting:**
- Adaptive timeouts based on content length
- Text stability detection (avoid premature extraction)
- No-change fallback (exit if truly stuck)

**Resource Management:**
- Reuse browser session across prompts
- Limit concurrency to 1 (avoid bans)
- Random human-like delays (0.7-1.0s between actions)

## Output Formats

### NDJSON (Structured Data)
```json
{"website_name":"KIMI","conversation_id":"19b0d87d-...","prompt_text":"推荐牛奶品牌","response_text":"以下推荐...[今日头条](http://...)","web_search_results":[{"href":"https://...","name":"什么值得买","title":"婴儿牛奶推荐","snippet":"..."}],"response_language":"zh","latency_ms":76255,"status":"ok"}
```

### Markdown (Human-Readable)
```markdown
# Conversation 19b0d87d-...

- **Website**: KIMI
- **Model**: 
- **Online Mode**: 
- **Language**: zh
- **Latency**: 76255 ms

## Prompt

推荐牛奶品牌

## Response

以下推荐...[今日头条](http://...)...

## Web Search Results

### 1. 婴儿牛奶推荐

- **URL**: https://www.smzdm.com/...
- **Site**: 什么值得买

---
```

## Key Selectors for Kimi.moonshot.cn

> ⚠️ **IMPORTANT**: The following selectors and implementation details are **specific to Kimi.moonshot.cn**. 
> Other AI chat services (DeepSeek, Doubao/豆包, ChatGPT, Qwen, etc.) will have **different UI structures and operational logic**.
> 
> **When adapting to a new site:**
> 1. Use MCP browser tools (`mcp_cursor-ide-browser_*`) to navigate and snapshot the target site
> 2. Analyze the page structure to identify equivalent elements
> 3. Test interaction patterns (send button behavior, response completion signals, etc.)
> 4. Document site-specific quirks and adjust the workflow accordingly

### Kimi-Specific Selectors

#### Input & Send
- Chat Input: `"div.chat-content-container div[role='textbox']"`
- Send Button: `'button[aria-label*="发送"]'`
- Send Button Root: `"div.send-button"`

#### Response Detection
- Stop Icon: `'div.send-button svg[name="stop"]'`
- Send Icon: `'div.send-button svg[name="Send"]'`
- Disabled State: `"div.send-button-container.disabled"`

#### Content Extraction
- Assistant Message: `'div.chat-content-item.chat-content-item-assistant'`
- Markdown Container: `"div.markdown-container"`
- Citation Tags: `".rag-tag"` (hover to reveal href)

#### Web Search
- Search Block: `"div.container-block"` with search icon
- Side Panel: `"div.side-console-container"`
- Search Results: `'a.site'` within side panel

### Site-Specific Behaviors (Kimi)

**Citation Extraction:**
- Kimi uses `div.rag-tag` that transforms to `a.rag-tag` on hover
- Requires JavaScript mouseover events to trigger href population
- Need 0.5-0.7s wait after batch hover for all transformations

**Response Completion:**
- Detects via send button state changes (Stop icon → Send icon)
- Text stability check: 3 ticks with no changes (3 × 0.4s = 1.2s)
- Fallback: 10s no-change timeout

**Web Search Results:**
- Clicking search summary opens side panel
- Results appear in `div.side-console-container.normal`
- Each result is an `<a>` tag with structured data attributes

**New Conversation:**
- Button has `svg[name="AddConversation"]`
- Creates fresh conversation with new URL/conversation_id
- Input box becomes available after ~1s

## Adapting to Other AI Chat Sites

### Discovery Process Using MCP Tools

When creating a scraper for a new AI chat service (e.g., DeepSeek, Doubao, ChatGPT), follow this process:

**Example Implementation**: A DeepSeek scraper has been created at `MCPfiles/deepseek_chat_scraper.py` using this process. It uses generic selectors that may need adjustment after testing with the actual site.

#### 1. Navigate and Snapshot
```python
# Use MCP browser tools to explore the site
mcp_cursor-ide-browser_browser_navigate(url="https://chat.deepseek.com")
snapshot = mcp_cursor-ide-browser_browser_snapshot()
# Analyze the snapshot to identify key elements
```

#### 2. Identify Key Elements
Look for equivalents of:
- **Input box**: `role="textbox"`, `contenteditable="true"`, `<textarea>`, etc.
- **Send button**: Look for icons, text, or `aria-label` containing "send", "发送", "提交"
- **Response container**: Usually a `<div>` or `<article>` with role="assistant" or class containing "message", "response"
- **Loading/generating indicator**: Spinner, "Stop" button, progress bar, or streaming cursor
- **Citations/sources**: Links, footnotes, or reference sections within the response

#### 3. Test Interaction Patterns

**Send Button Behavior:**
- Does it require focus + type + click, or just type + Enter?
- Are there keyboard shortcuts (Shift+Enter for newline)?
- Does it disable during generation?

**Response Completion Detection:**
- How to know when streaming ends? (Stop button disappears? Regenerate appears? Text stops changing?)
- What's the typical response time range?
- Are there rate limits or anti-bot challenges?

**Citations/Sources:**
- Are they inline links `<a>` or separate sections?
- Do they require hover/click to reveal URLs?
- Is there a "web search" toggle or mode?

**Session Management:**
- Does it use cookies, localStorage, or other storage?
- How long do sessions last?
- Can sessions be reused across runs?

#### 4. Document Differences

Create a comparison table:

| Feature | Kimi | DeepSeek | Doubao | Your Site |
|---------|------|----------|--------|-----------|
| Input selector | `div[role='textbox']` | `textarea._27c9245` ✅ | ? | ? |
| Send method | Click button | Press Enter ✅ | ? | ? |
| Stop icon | `svg[name="stop"]` | `button:has-text("停止")` | ? | ? |
| Response container | `.chat-content-item-assistant` | `div.ds-message._63c77b1` ✅ | ? | ? |
| Message list | `div.chat-content-list` | `div.dad65929` ✅ | ? | ? |
| Markdown container | `div.markdown-container` | `div.ds-markdown` ✅ | ? | ? |
| Citation format | Hover `.rag-tag` → `<a>` | Direct `<a[target="_blank"]>` ✅ | ? | ? |
| Web search button | `div.container-block` | `div._74c0879` ✅ | ? | ? |
| Web search panel | `div.side-console` | `div._519be07` / `div.dc433409` ✅ | ? | ? |
| New conversation | `svg[name="AddConversation"]` | `div._5a8ac7a:has-text("开启新对话")` ✅ | ? | ? |

**Note**: DeepSeek selectors marked with ✅ are verified from actual page structure (2025-12-11).

#### 5. Adjust Code Template

**Example adaptations needed:**

```python
# For DeepSeek (hypothetical):
CHAT_INPUT_SELECTORS = [
    "textarea.chat-input",           # Different from Kimi
    'div[contenteditable="true"]'
]

SEND_BUTTON_SELECTORS = [
    'button:has(svg.icon-send)',     # Different icon structure
    'button[type="submit"]'
]

# Response completion might use different logic:
def is_generating_deepseek(page):
    # DeepSeek might use a different indicator
    return page.locator('.loading-spinner').is_visible()

# Citations might be already in <a> tags (no hover needed):
def extract_citations_deepseek(container):
    links = container.locator('a[href^="http"]').all()
    return [link.get_attribute('href') for link in links]
```

### Common Variations Across Sites

**Input Methods:**
- Kimi: `contenteditable div`
- ChatGPT: `<textarea>`
- Some sites: Rich text editor with complex DOM

**Completion Signals:**
- Kimi: Stop icon disappears + Send icon appears
- ChatGPT: "Regenerate" button appears
- Some sites: WebSocket close event or API response complete

**Citation Patterns:**
- Kimi: Hover-to-reveal inline tags
- ChatGPT: Numbered footnotes `[1]` with link section
- Some sites: Separate "Sources" expandable section

**Rate Limiting:**
- Kimi: Soft limits, longer waits between prompts
- Some sites: Hard captcha after N requests
- Some sites: Account-based quotas

### MCP Tools Reference

Use these MCP tools during site analysis:

- `mcp_cursor-ide-browser_browser_navigate`: Go to target URL
- `mcp_cursor-ide-browser_browser_snapshot`: Get accessibility tree (better than screenshot for structure)
- `mcp_cursor-ide-browser_browser_click`: Test clicking elements
- `mcp_cursor-ide-browser_browser_type`: Test typing into input
- `mcp_cursor-ide-browser_browser_wait_for`: Wait for text/elements to appear/disappear
- `mcp_cursor-ide-browser_browser_console_messages`: Check for JavaScript errors
- `mcp_cursor-ide-browser_browser_network_requests`: Analyze API calls

### Validation Checklist

Before finalizing a scraper for a new site, verify:

- [ ] Can detect login state reliably
- [ ] Input box and send button work consistently
- [ ] Response completion detection is accurate (not too early/late)
- [ ] Citations/sources are fully extracted
- [ ] Error handling covers common failures
- [ ] Session persistence works across runs
- [ ] No hard anti-bot blocks (or has workarounds)
- [ ] Respects rate limits and ToS

## Best Practices

### DO:
✓ Use persistent session files to reduce login frequency
✓ Maximize browser window for better debugging visibility
✓ Save results after EACH prompt (crash-resistant)
✓ Start new conversation for each prompt (clean context)
✓ Use JavaScript batch operations for citations (faster)
✓ Implement prompt reuse detection (skip processed ones)
✓ Support multi-task workflows (task1, task2, etc.)
✓ Extract structured web search results (not just URLs)
✓ Convert response to Markdown with inline citations
✓ Add random human-like delays between actions
✓ Handle both Chinese and English interfaces

### DON'T:
✗ Hardcode fixed wait times (use dynamic detection)
✗ Process all prompts in one long conversation
✗ Save results only at the end (risk data loss)
✗ Ignore citation transformation timing
✗ Skip error handling for network issues
✗ Use brittle CSS class selectors
✗ Forget to check for web search mode
✗ Extract plain text without citation context
✗ Process already-completed prompts
✗ Run multiple tasks in parallel (rate limits)

## Troubleshooting

**Citations not extracting:**
- Increase hover wait time (0.5s → 0.7s)
- Check if JavaScript events triggered
- Verify `.rag-tag` → `a.rag-tag` transformation
- Try multiple hover rounds

**Response truncated:**
- Increase text stability threshold
- Add fallback timeout (10s no-change)
- Check if "Stop" icon still visible

**Login detection fails:**
- Increase manual login wait time (15s → 30s)
- Check alternative input box selectors
- Verify localStorage language settings

**New conversation button not found:**
- Try alternative selectors
- Check button visibility timeout
- Ensure previous conversation finished

**Rate limiting:**
- Increase delays between prompts
- Add exponential backoff
- Reduce concurrent requests to 1



# Anti bot countermeasures
- In case Akamai is protecting the website, implement scrapy_impersonate on the scraper using these custom options: 
  custom_settings = {
		"DOWNLOAD_HANDLERS": {
			"http": "scrapy_impersonate.ImpersonateDownloadHandler",
			"https": "scrapy_impersonate.ImpersonateDownloadHandler",
			},
		"TWISTED_REACTOR": "twisted.internet.asyncioreactor.AsyncioSelectorReactor",
	}
  and then add 'impersonate':'chrome110' in the meta values of each request.

# Suggested settings for Chatbox/Camoufox scrapers
- Use a persistent user data dir to preserve login and reduce repeated challenges
- Limit concurrency to 1 and add random think-time between actions
- Prefer visible, stable selectors (aria-label, role, data-testid) over brittle CSS classes
- Headful mode is recommended during selector discovery; switch to headless for production if stable
- Respect site ToS and avoid automated scraping if prohibited; stop on hard anti-bot that requires human solve

# Anti bot countermeasures for Chatbox sites
- If Akamai simple checks appear, prefer Camoufox with up-to-date browser binary and realistic headers
- If Datadome or Kasada challenges appear, stop the process
- Handle soft rate limits with exponential backoff and retry after headers
  